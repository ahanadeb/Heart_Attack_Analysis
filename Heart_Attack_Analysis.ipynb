{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('HA_analysis/heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ha_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 13   \n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTrainDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data_x = self.landmarks_frame\n",
    "        data_x = data_x.to_numpy()\n",
    "        #labels = np.transpose(data_x[:,13]);\n",
    "        labels = np.transpose(data_x[0:270,14]);\n",
    "    \n",
    "        labels = labels.astype('float')\n",
    "        \n",
    "        \n",
    "        #features=data_x[:,0:13];\n",
    "        features=data_x[0:270,1:14];\n",
    "        features = features.astype('float')\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample = {'features': features, 'labels': labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTestDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data_x = self.landmarks_frame\n",
    "        data_x = data_x.to_numpy()\n",
    "        #labels = np.transpose(data_x[:,13]);\n",
    "        labels = np.transpose(data_x[270:304,14]);\n",
    "    \n",
    "        labels = labels.astype('float')\n",
    "        \n",
    "        \n",
    "        #features=data_x[:,0:13];\n",
    "        features=data_x[270:304,1:14];\n",
    "        features = features.astype('float')\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample = {'features': features, 'labels': labels}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        features, labels = sample['features'], sample['labels']\n",
    "        #print(\"l\")\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        return {'features': torch.from_numpy(features),\n",
    "                'labels': torch.from_numpy(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = GetTrainDataset(csv_file='ha_data.csv',\n",
    "                                           \n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GetTestDataset(csv_file='ha_data.csv',\n",
    "                                           \n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/303], Loss: 0.4433\n",
      "Epoch [1/10], Step [200/303], Loss: 0.3593\n",
      "Epoch [1/10], Step [300/303], Loss: 0.3288\n",
      "Epoch [2/10], Step [100/303], Loss: 0.3063\n",
      "Epoch [2/10], Step [200/303], Loss: 0.2824\n",
      "Epoch [2/10], Step [300/303], Loss: 0.2522\n",
      "Epoch [3/10], Step [100/303], Loss: 0.2635\n",
      "Epoch [3/10], Step [200/303], Loss: 0.1844\n",
      "Epoch [3/10], Step [300/303], Loss: 0.1597\n",
      "Epoch [4/10], Step [100/303], Loss: 0.1650\n",
      "Epoch [4/10], Step [200/303], Loss: 0.1140\n",
      "Epoch [4/10], Step [300/303], Loss: 0.1055\n",
      "Epoch [5/10], Step [100/303], Loss: 0.0815\n",
      "Epoch [5/10], Step [200/303], Loss: 0.0658\n",
      "Epoch [5/10], Step [300/303], Loss: 0.0587\n",
      "Epoch [6/10], Step [100/303], Loss: 0.2184\n",
      "Epoch [6/10], Step [200/303], Loss: 0.1443\n",
      "Epoch [6/10], Step [300/303], Loss: 0.1144\n",
      "Epoch [7/10], Step [100/303], Loss: 0.0920\n",
      "Epoch [7/10], Step [200/303], Loss: 0.0775\n",
      "Epoch [7/10], Step [300/303], Loss: 0.0644\n",
      "Epoch [8/10], Step [100/303], Loss: 0.3231\n",
      "Epoch [8/10], Step [200/303], Loss: 0.0704\n",
      "Epoch [8/10], Step [300/303], Loss: 0.0577\n",
      "Epoch [9/10], Step [100/303], Loss: 0.0488\n",
      "Epoch [9/10], Step [200/303], Loss: 0.0419\n",
      "Epoch [9/10], Step [300/303], Loss: 0.0367\n",
      "Epoch [10/10], Step [100/303], Loss: 0.0324\n",
      "Epoch [10/10], Step [200/303], Loss: 0.0278\n",
      "Epoch [10/10], Step [300/303], Loss: 0.0238\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_step = len(dataloader)\n",
    "curr_lr = learning_rate\n",
    "loss_vals=  []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss= []\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        #images = images.reshape(-1, input_size)\n",
    "        features=sample_batched['features'];\n",
    "        labels=sample_batched['labels'];\n",
    "        # Forward pass\n",
    "        \n",
    "        features = features.reshape(-1, input_size)\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        \n",
    "        outputs = model(features)\n",
    "        #print(outputs)\n",
    "        #print(labels.shape)\n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        #print(\"a\")\n",
    "        if (i_batch+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "    \n",
    "    loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 87 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        features=sample_batched['features'];\n",
    "        labels=sample_batched['labels'];\n",
    "        features = features.reshape(-1, input_size)\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Test set accuracy: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
